\section{Probability theory}

%1
\begin{frame}
    \frametitle{A bit of history}
\begin{block}{Games of chance}
    \begin{itemize}
        \item<+-> According to popular belief, knights returning from the Crusades introduced a dice game named "Hazard."
         Although the rules were complicated, the game attracted many players.
        \item<+-> The mathematician Gerolamo Cardano, in the middle of the 16th century, starts to investigate the odds of winning
        in a game of chance. He gives the first definition of the probability of an event as the ratio of
        the number of favorable outcomes to the total number of outcomes.
        \item<+-> One century later, Pierre de Fermat and Blaise Pascal lay the first theory of probability when
        answering a question from Antoine Gombaud, a famous Parisian gambler.
    \end{itemize}
\end{block}
\end{frame}
%2
\begin{frame}
    \frametitle{A bit of history}
\begin{block}{A modern view}
    \begin{itemize}
        \item<+-> In 1933, the Russian mathematician Andrey Kolmogorov gives the first axiomatic description of probability theory.
        \item<+-> This is the approach we will take in the sequel.
    \end{itemize}
\end{block}    
\begin{block}{Challenges}
    \begin{itemize}
       \item<+-> To fully understand Kolmogorov's contribution, we must consider cases in which counting the number of favorable outcomes is insufficient.
       \item<+-> For example, take a situation where the measure of interest is a real number, such as a current or a pressure.
       \item<+-> The value itself is not discrete; only the number of times it falls within a given interval can be observed.
       \item<+-> The Fermat-Pascal approach must be extended to these cases.
    \end{itemize}
\end{block}    
\end{frame}
%3
\begin{frame}
    \frametitle{From intuition to axioms }
\begin{block}{Sample space}
    \begin{itemize}
        \item<+-> Let's consider a very simple experiment: a coin is tossed, and the outcome observed.
        \item<+-> What are the possible results ?
        \item<+-> Of course, "head" and "tails" !
        \item<+-> The \textbf{sample space} of an experiment is the set of all possible results.
        \item<+-> In the coin tossing example, it is just the set $\Omega = \{ \text{heads}, \text{tails}\}.$
        \item<+-> If a die is rolled, the sample space is $\Omega = \{ 1,2,3,4,5, 6\}.$
        \item<+-> In a physical system, $\Omega$ may be a subset of the real numbers,
        or something even more complicated.
    \end{itemize}
\end{block}    
\end{frame}
%4 
\begin{frame}
    \frametitle{From intuition to axioms}
\begin{block}{Events}
    \begin{itemize}
        \item<+-> If two coins are tossed, the sample space is 
        $\Omega = \{ HH, HT, TH, TT \}.$
        \item<+-> An \textbf{event} is a statement about the outcome of the experiment, 
        like "at least one tails was observed".
        \item<+-> The previous event is the subset $\{HT,TH,TT\}$ of $\Omega.$
        \item<+-> In probability theory, an \textbf{event} is a subset of the \textbf{sample space}.
        \item<+-> The empty set $\emptyset$ and the sample space $\Omega$ are 
        always considered as events.
    \end{itemize}
\end{block}
\end{frame}
%5 
\begin{frame}
    \frametitle{From intuition to axioms}
\begin{block}{A bit of logic}
    \begin{itemize}
        \item<+-> Assertions about events are also events:
        \begin{itemize}
            \item<+-> If $A,B$ are events, so is $A \cup B.$
            \item<+-> If $A$ is an event, the complementary set $\bar{A}$ is an event.
        \end{itemize}
        \item<+-> For the two coins tossing experiment, if $A=\{HH\},B=\{HT\}$, then
         the event $A \cup B = \{HH, HT\}$ is "the first toss is a head", while the event
        $\bar{A}=\{HT,TH,TT\}$ is "at least one tails was observed".
    \end{itemize}
\end{block}
\begin{block}{Exercise}
    \begin{itemize}
        \item<+-> Find the sample space $\Omega$ associated with the roll of a die.
        \item<+-> What are the respective subsets $A,B$ of $\Omega$ corresponding to the observations
        "the result is greater than or equal to 3","the result is an even number"?
        \item<+-> Describe the events $A\cup B$, $\bar{A}.$
    \end{itemize}
\end{block}
\end{frame}
%6
\begin{frame}
    \frametitle{Probabilities}
    \begin{block}{$\sigma$-algebras}
        \begin{itemize}
            \item<+-> Given a sample space $\Omega$, a $\sigma$-algebra of
            events is a set $\mathcal{T}$ of subsets from $\Omega$ such that:
            \begin{itemize}
                \item<+-> $\Omega \in \mathcal{T}.$
                \item<+-> If $A \in \mathcal{T}$, then $\bar{A} \in \mathcal{T}.$
                \item<+-> For any \textbf{countable} sequence $A_n$ in $\mathcal{T}$, the union
                $\bigcup_n A_n$ is in $\mathcal{T}.$
            \end{itemize}
        \end{itemize}
    \end{block}
\begin{block}{Probability measure}
    \begin{itemize}
        \item<+-> Given a $\sigma$-algebra $\mathcal{T}$ on $\Omega$, a 
        probability $P$ is a mapping that assigns to $A \in \mathcal{T}$ a real
        number $P(A) \in [0,1]$ such that:
        \begin{itemize}
        \item<+-> $P\left( \Omega \right) = 1.$
        \item<+-> For any countable sequence $A_n$ of \textbf{pairwise disjoint} events in $\mathcal{T}$,
        \begin{equation}
             \label{eq:proba_additivty}
            P\left( \bigcup_n A_n \right) = \sum_n P\left( A_n \right).
        \end{equation}
        \end{itemize}
    \end{itemize}
\end{block}
\end{frame}
%7
\begin{frame}
    \frametitle{Some properties}
\begin{block}{Probability space}
A sample space $\Omega$ together with a $\sigma$-algebra of events $\mathcal{T}$ and
a probability $P$ is called a \textbf{probability space}, denoted by 
$\left( \Omega, \mathcal{T}, P \right).$
\end{block}
\begin{block}{Set operations}
    Let $\left( \Omega, \mathcal{T}, P \right)$ be a probability space. 
    Let $A,B$ be events. Then:
    \begin{itemize}
        \item<+-> $P\left(\bar{A} \right) = 1 - P(A).$
        \item<+-> $P\left( A \cup B \right) = P(A) + P(B) - P\left( A \cap B \right).$
        \item<+-> If $B \subset A$, then $P\left( A - B \right) = P(A) - P(B)$, where
        $A-B = A \cap \bar{B}.$
    \end{itemize}
\end{block}
\end{frame}
%8
\begin{frame}
    \frametitle{The die roll}
\begin{block}{Defining a Probability}
    \begin{itemize}
        \item<+-> $\Omega = \{1,2,3,4,5,6\}$, $\mathcal{T} = \mathcal{P}\left( \Omega \right)$, 
        the $\sigma$-algebra of all subsets. 
        \item<+-> How to find a probability on $\mathcal{T}$ ?
        \item<+-> It is required that $P\left( \Omega \right) = 1.$
        \item<+-> If the die is fair, then the probability to observe any number
        from $\Omega$ must be the same.
        \item<+-> Since:
        \begin{equation}
            1 = P\left( \Omega \right) = \sum_{i=1}^6 P\left( \{i\} \right),
        \end{equation}
        one deduces $P\left( \{i\} \right) = 1/6, i=1 \dots 6.$
        \item<+-> The probability of any event can then be obtained by summing the probabilities
        of its elements. 
    \end{itemize}
\end{block}
\end{frame}
%9
\begin{frame}
    \frametitle{Exercise}
\begin{block}{A card game}
   \begin{itemize}
    \item<+-> A gambler randomly draws a hand of 5 cards from a deck of 52. Can you find 
    a sample space describing this experiment ?
    \item<+-> If the probabilities of all hands are equal, what is the probability of having a four of a kind ?
    \item<+-> Hint: the number of ways to select $k$ elements in a set of $n$
    is :
    \begin{equation}
        \begin{pmatrix}
            n \\ k
        \end{pmatrix} = \frac{n!}{k!\left( n-k \right)!}.
    \end{equation}
   \end{itemize} 
\end{block}
\end{frame}
%10
\begin{frame}
    \frametitle{Conditional probability}
    \begin{block}{Motivation}
    \begin{itemize}
        \item<+-> In many cases, there is some information about the outcome of
        an experiment.
        \item<+-> As an example, for a die roll, it may be "the result is odd".
        \item<+-> The sample space is thus reduced, and the probabilities must 
        be rescaled accordingly.
        \item<+-> For the die example, it is $\{1,3,5\}$, and the probability 
        to draw a $3$ is $1/3.$
    \end{itemize}
    \end{block}
    \begin{block}{Bayes' formula}
        The conditional probability of $A$ knowing $B$, denoted $P\left( A \vert B \right)$, is given by the Bayes' formula:
        \begin{equation}
            P\left( A \vert B  \right) = \frac{P\left( A \cap B \right)}{P(B)}.
        \end{equation}
    \end{block}
\end{frame}
%11
\begin{frame}
    \frametitle{Around Bayes' formula}
    \begin{block}{Useful formulas}
    \begin{itemize}
        \item<+-> Let $\left(B_n\right)$ be a countable sequence of pairwise disjoint events such that $\bigcup_n B_n = \Omega.$ For any event $A$,
        one has the formula of total probabilities:
        \begin{equation}
            P\left( A \right) = \sum_n P\left( A \vert B_n \right) P\left( B_n \right).
        \end{equation}
        \item<+-> Given events $B_1 \dots B_n, A$, one has:
        \begin{equation}
            P\left( A \right) = P\left( A \vert B_1, \dots, B_n \right)P(B_n \vert B_{n-1}, \dots B_1)\dots P\left( B_1 \right).
        \end{equation}
        \item<+-> Conditioning can be reversed using the next formula, provided $P(B) \neq 0:$
        \begin{equation}
            P\left( B \vert A \right) = \frac{P\left( A \vert B \right) P(B)}{P(A)}
        \end{equation}
       \end{itemize}
    \end{block}
\end{frame}
%12
\begin{frame}
    \frametitle{Exercise}
Two factories, denoted $F_0, F_1$ are producing components.  The end customer 
receives a component that comes from $F_0$ with probability $0.3$. Furthermore,
the probability of the component being defective is $0.1$ when coming from factory $F_0$
and $0.06$ when coming from factory $F_1$. If the recived component is defective,
what is the probability that it has been produce by $F_1$ ?

\end{frame}
%13
\begin{frame}
    \frametitle{Random variables}
\begin{block}{Definition}
    Given two sample spaces $E,F$ equipped with respective $\sigma$-algebras of events 
    $\mathcal{T},\mathcal{F}$, a mapping $X \colon E \to F$ is said to be 
    a random variable if, for any event $A \in \mathcal{F}$, $X^{-1}\left( A \right)$
    is an event of $\mathcal{T}.$
\end{block}
\begin{block}{Example}
    Consider a die roll with $E=\{1,2,3,4,5,6\}$ and 
    $\mathcal{T}=\{E,\emptyset,\{2,4,6\},\{1,3,5\}\}$. Let $F=\{0,1\}, \mathcal{F}=\mathcal{P}(F).$
    The mapping $X$ that associates to an even number the value 0 and 1 to an odd number
    is a random variable. However, $Y$ that maps any value less than 3 to 0 and the
    others to 1 is not.
\end{block}
\end{frame}
%14
\begin{frame}
    \frametitle{The law of a random variable}
\begin{block}{DÃ©finition}
    \begin{itemize}
        \item<+-> In the previous example,  
        the probability of the event $X=0$ is $1/2$, the probability of  
        $X^{-1}\left( 0 \right)=\{2,4,6\}.$
        \item<+-> If $X$ is a random variable on a probability space $\left( E,\mathcal{T},P \right)$
        with values in $F$, the law of $X$ is the probability on $\mathcal{F}$ defined by:
        \begin{equation}
            P_X \left( A \right) = P \left( X^{-1}(A) \right).
        \end{equation}
    \item<+-> In many cases, the outcome of an experiment is impossible to observe, but 
    some agregated value is, e.g. the pressure or the temperature of a gas.
    \item<+-> Formally, this is a random variable.
    \item<+-> Only its law has some interest.
   \end{itemize} 
\end{block}    
\end{frame}

%15
\begin{frame}
    \frametitle{Discrete random variables}
\begin{block}{The law of a discrete random variable}
    \begin{itemize}
        \item<+-> A random variable is said to be discrete if it takes its 
        values in a finite or infinite countable set.
        \item<+-> The law of such a random variable $X$ with values in a set $E$ is entirely determined
        by its \textbf{distribution} $p(x) = P_X\left( \{x\} \right), x \in E.$
    \end{itemize}
\end{block}
\begin{block}{Expected value}
    \begin{itemize}
        \item<+-> Let $X$ be a discrete random variable with values in $E\subset \R$. 
        Its expected value is defined as:
        \begin{equation}
            E\left[ X \right] = \sum_{x \in E} x p(x),
        \end{equation}
        provided:
        \begin{equation}
            \sum_{x \in E} \lvert x \rvert p(x)< +\infty.
         \end{equation}
    \end{itemize}
\end{block}
\end{frame}
%16
\begin{frame}
    \frametitle{Exercise}
\begin{block}{A game of chance}
    \begin{itemize}
        \item<+-> Two dice are rolled and their values are added.
        \item<+-> Show that the sum is a random variable $X$ with values in 
        $\{2, \dots, 12 \}.$
        \item<+-> Find the distribution of $X$.
        \item<+-> If you bet at the start, you'll win five times
         your original bet if $X \geq 10.$ Is this rule fair?
    \end{itemize}
\end{block}
\end{frame}
%17
\begin{frame}
    \frametitle{Exercise}
\begin{block}{Bernoulli random variable}
    \begin{itemize}
        \item<+-> A bernoulli random variable $X$ can take only two values: $0$ and $1$.
        \item<+-> If $P\left( X=1 \right)  = p, $ can you compute $P\left( X=0 \right) ?$
        \item<+-> Compute $E\left[ X \right].$
    \end{itemize}
\end{block}
\begin{block}{Binomial random variable}
    \begin{itemize}
        \item<+-> A binomial random variable $X$ with parameters $n \in \N, p \in [0,1]$ has 
        distribution:
        \begin{equation}
            p(k) = \begin{pmatrix}
                n \\ k
            \end{pmatrix} p^k \left( 1-p \right)^{n-k}, k = 0 \dots n.
        \end{equation}
        \item<+-> Find $E\left[ X \right].$
    \end{itemize}
\end{block}

\end{frame}
%18
\begin{frame}
    \frametitle{Expected value}
\begin{block}{Properties}
    \begin{itemize}
        \item<+-> If $X,Y$ are discrete random variables with values in $E \subset \R$ and 
        $\lambda$ is a real number, then:
        \begin{equation}
            E\left[ \lambda X + Y \right] = \lambda E\left[ X \right] + E \left[ Y \right].
        \end{equation}
        \item<+-> The expected value is monotone. If $X \leq Y$, then:
        \begin{equation}
            E\left[ X \right] \leq E \left[ Y \right]
        \end{equation}
       
    \end{itemize}
\end{block}
\end{frame}
%19
\begin{frame}
    \frametitle{Moments}
\begin{block}{The expected value in a general sense}
    \begin{itemize} 
        \item<+-> If $g \colon \R \to \R$ is piecewise continuous and
        \begin{equation}
            \sum_{x \in E} \lvert g(x) \rvert p(x) < +\infty,
        \end{equation}
        then one defines:
        \begin{equation}
            E\left[ g(X) \right] = \sum_{x \in E}  g(x) p(x)
        \end{equation}
    \item<+-> The variance of the discrete random variable $X$ is:
    \begin{equation}
        V(X) = E \left[ \left( X - E\left[ X \right] \right)^2 \right]
    \end{equation}
    \item<+-> It measures the dispersion of $X$ around its expected value.
    \end{itemize}
\end{block}
\end{frame}
%20
\begin{frame}
    \frametitle{Moments}
\begin{block}{Higher order moments}
    \begin{itemize} 
        \item<+-> Let $n$ be an integer. The $n$-th moment of $X$ is, if it
        exists:
        \begin{equation}
            E\left[ X^n \right].
        \end{equation}
        \item<+-> The moment generating function is the function:
        \begin{equation}
            t \mapsto m_X(t) = E\left[ e^{tX} \right]
        \end{equation}
    \item <+-> If its domain contains $0$, then:
    \begin{equation}
        E\left[ X^n \right] = m^{(n)}(0),
    \end{equation}
    where $m^{(n)}$ denotes the $n$-th derivative of $m_X$.
\end{itemize}
\end{block}
\end{frame}
%21
\begin{frame}
    \frametitle{Conditioning}
\begin{block}{Independence}
    \begin{itemize}
        \item<+-> By the Bayes' formula:
        \begin{equation}
            P\left( X = x \vert Y = y \right) = \frac{P\left( X = x, Y = y \right)}{P(Y=y)}.
        \end{equation}
        \item<+-> If $P\left( X = x, Y = y \right) = P\left( X = x \right)P\left( Y = y \right)$, then
        $P\left( X = x \vert Y = y \right) = P\left( X = x \right).$
        \item<+-> In such a case, $X,Y$ are said to be \textbf{independent}.
        \item<+-> If $X,Y$ are independent, then $E\left[ XY \right] = E[X]E[Y].$
        \item<+-> If $X_1, \dots, X_n$ are pairwise independent, then:
        \begin{equation}
            V\left( X_1 + \dots + X_n \right) = V\left( X_1 \right) + \dots + V\left( X_n \right).
        \end{equation}
    \end{itemize}
\end{block}
\end{frame}
%22
\begin{frame}
    \frametitle{Exercise}
    \begin{block}{A pair of dice... again !}
        \begin{itemize}
            \item<+-> Let $X,Y$ be two independent random variables corresponding
            to the respective values of two die throws. Compute~:
            \[
            E\left[ X \right], \, E\left[ Y \right], \, E\left[ X + Y \right].
            \]
            \item<+-> Same question with the variances.
        \end{itemize}
    \end{block}
    \begin{block}{A taste of estimation theory}
        \begin{itemize}
            \item<+-> Let $X_1, \dots X_n$ be a sequence of independent random variables and
            let:
            \[
            \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i.
            \]
            \item<+-> Compute $E\left[ \bar{X} \right], V\left( \bar{X} \right).$ What happens 
            if $n \to +\infty ?$
        \end{itemize}
    \end{block}    
\end{frame}

\begin{frame}
    \frametitle{Poisson distribution}
\begin{block}{A model for random events}
   \begin{itemize}
    \item<+-> Given a time interval of length $\left[ t_0, t_1 \right]$, one counts the number of occurrences $X$ of an event.
    \item<+-> The random variable $X$ is said to have a Poisson distribution with rate $\lambda$ if:
    \begin{equation}
        P\left( X=k \right) = \frac{\left( \lambda T \right)^k}{k!} e^{-\lambda T},
    \end{equation}
    where $T=t_1-t_0.$
    \item<+-> $E\left[ X \right] = V(X)= \lambda T.$
    \item<+-> The Poisson distribution models a situation where the occurrences of an event are independent and occur at constant rate.
   \end{itemize} 

\end{block}
\end{frame}

\begin{frame}
    \frametitle{Exercise}
\begin{block}{Distribution of a sum}
    \begin{itemize}
        \item<+-> The moment generating function (MGF) of a random variable is the function $m_X(t)=E\left[ e^{tX} \right].$ It is
        characteristic of a distribution.  Prove that if $X,Y$ are independent, $m_{X+Y}(t)=m_X(t)m_Y(t).$
        \item<+-> Compute the MGF of $X$ with Poisson distribution of rate $\lambda.$
        \item<+-> Let $X,Y$ be independent random variables with Poisson distributions of respective rates $\lambda,\mu.$ 
        Show that $X+Y$ has Poisson distribution of rate $\lambda+\mu.$
    \end{itemize}
\end{block}
\begin{block}{Selecting events}
    Let $X$ be a random variable with Poisson distribution of rate $\lambda$. The underlying events are selected at random
    with probability $p$ and counted, yielding a random variable $Y$. Prove that $Y$ has Poisson distribution with rate $p \lambda.$
\end{block}    

\end{frame}

\begin{frame}
    \frametitle{Real random variables}
\begin{block}{Definition}
    \begin{itemize}
    \item<+-> Let $\left( \Omega, \mathcal{T}, P \right)$ be a measure space. A mapping $X \colon \Omega \to \R$ is
    said to be a real random variable if, for any $t \in \R$, $\{ \omega \vert X(\omega) \leq t \}$ is an event in $\mathcal{T}.$
    \item<+-> If $X$ is a real random variable, its cumulative distribution function (CDF) is the mapping:
    \begin{equation}
        F_x \colon t \in R \mapsto P\left( X \leq t \right).
    \end{equation}
    \item<+-> $\lim_{t \to -\infty}F_x(t) = 0, \, \lim_{t \to +\infty} F_x(t)=1.$
    \item<+-> $F_x$ is right continuous and admits a limit to the left.
    \item<+-> If $F_x$ is differentiable, its derivative is called the density of $X$.
    \end{itemize}
\end{block}
\end{frame}
\begin{frame}
    \frametitle{Real random variables}
\begin{block}{Expected value}
    \begin{itemize}
    \item<+-> Let $X$ be a real random variable with density $p_X$. 
    \item<+-> If $g \colon \R \to \R$ is piecewise continuous, then one defines:
    \begin{equation}
        E\left[ g(X) \right] = \int_{\R} g(t) p_X(t) dt
    \end{equation} 
    provided:
\begin{equation}
    \int_{\R} \lvert g(t) \rvert p_X(t) dt < +\infty.
\end{equation}
    \item<+-> As special cases:
    \begin{equation}
        \begin{split}
        & P\left( X \in [a,b] \right) = E \left[ 1_{[a,b]}(X) \right] = \int_a^b p_X(t) dt \\
        & F_X(t) = \int_{-\infty}^t p_X(t) dt.
        \end{split}
    \end{equation}
\end{itemize}
\end{block}
\end{frame}
\begin{frame}
    \frametitle{Real random variables}
\begin{block}{Joint CDF and density}
    \begin{itemize}
        \item<+-> If $X,Y$ are real random variables, the CDF of the couple $(X,Y)$ is the mapping:
        \begin{equation}
            F_{X,Y} \colon (s,t) \in \R^2 \mapsto P\left( X \leq s, Y \leq t \right).
        \end{equation}
        \item<+-> Taking the derivative with respect to $s,t$ allows defining the joint density:
        \begin{equation}
            p_{X,Y}(x,y)= \frac{\partial^2}{\partial_s \partial_t} F_{X,Y}(t,s) \vert_{x,y}.
        \end{equation}
        \item<+-> $X,Y$ are said to be independent if:
        \begin{equation}
            F_{X,Y}(s,t) = F_X(s) F_Y(t). 
        \end{equation}
    \end{itemize}
\end{block}
\end{frame}
\begin{frame}
    \frametitle{Marginal densities}
   \begin{itemize}
    \item<+-> Let $X,Y$ be a couple of real random variables with joint density $p_{X,Y}.$
    \item<+-> The density of $X$ (resp. $Y$) can be obtained as:
    \begin{equation}
        p_X(x) = \int_{\R} p_{X,Y}(x,t) dt \text{ (resp.) } p_Y(y) = \int_{\R} p_{X,Y}(s,y) ds
    \end{equation}
    \item<+-> The conditional density of $X$ knowing $Y$ is defined as:
    \begin{equation}
        p\left(X\vert Y=y\right)(s) = \frac{p_{X,Y}(s,y)}{p_Y(y)}.
    \end{equation}
    \item<+-> If $X,Y$ are independent, $p(X \vert Y=y)(s) = p_X(s)$ or, equivalently,
    $p_{X,Y}(x,y)=p_X(x)p_Y(y).$
   \end{itemize} 
\end{frame}
\begin{frame}
    \frametitle{Characteristic function}
    \begin{block}{Definition}
       \begin{itemize}
        \item<+-> Let $X$ be a real random variable. Its characteristic function is:
        \begin{equation}
            \phi_X \colon t \in \R \mapsto E\left[ e^{i t X} \right].
        \end{equation}
        \item<+-> Using the density $p_X$, it can be computed as:
        \begin{equation}
            \phi_X(t)  \int_{\R} e^{i t x} p_X(x) dx.
        \end{equation}
       \end{itemize} 
    \end{block}
    \vskip -24pt
    \begin{block}{Properties}
        \begin{itemize}
            \item<+-> If two random variables have the same characteristic function, they have the same distribution.
            \item<+-> If $X,Y$ are independent, $\phi_{X+Y}(t)=\phi_X(t)\phi_Y(t).$
            \item<+-> $E\left[ X^k \right] = i^{-k} \phi_X^{(k)}(0), k \in \N.$
        \end{itemize}
    \end{block}
\end{frame}
\begin{frame}
    \frametitle{Uniform distribution}
\begin{itemize}
    \item<+-> The uniform distribution on an interval $[a,b]$ has density:
    \begin{equation}
        p(x)= \begin{cases}
            \frac{1}{b-a} & x \in [a,b], \\
            0 & \text{ otherwise.}
        \end{cases}
    \end{equation}
    \item<+-> If $X$ has uniform distribution on $[a,b]$, then:
    \begin{equation}
        \begin{split}
            &E\left[ X \right] = \frac{a+b}{2}, \, V(X) = \frac{(b-a)^2}{12}, \\
            &\phi_X(t) = \frac{e^{itb}-e^{ita}}{t(b-a)}.
        \end{split}
    \end{equation}
\end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Normal distribution}
\begin{block}{Density}
    \begin{itemize}
        \item<+-> Let $\mu \in \R, \sigma > 0.$ A real random variable $X$
        is said to have a normal distribution $\mathcal{N}(\mu,\sigma^2)$ if its
        density is:
        \begin{equation}
            p_X(t)=\frac{1}{\sqrt{2 \pi}\sigma}e^{-\frac{(t-\mu)^2}{2 \sigma^2}}.
        \end{equation}
        \item<+-> If $X \sim \mathcal{N}(\mu,\sigma^2)$, then:
        \begin{equation}
            Y = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1).
        \end{equation}
    \end{itemize}
\end{block}
\end{frame}
\begin{frame}
    \frametitle{Normal distribution}
\begin{block}{Moments}
    If $X\sim \mathcal{N}\left(\mu, \sigma^2\right):$
    \begin{equation}
        \begin{split}
            &E\left[ X \right] = \mu, \, V\left( X \right) = \sigma^2,\\
            & \phi_X(t)= \exp\left( i \mu t - \sigma^2 t^2 / 2 \right).
        \end{split}
    \end{equation}
\end{block}
    

\end{frame}