\section{Probability theory}

%1
\begin{frame}
    \frametitle{A bit of history}
\begin{block}{Games of chance}
    \begin{itemize}
        \item<+-> According to popular belief, knights returning from the Crusades introduced a die game named "Hazard."
         Although the rules were complicated, the game attracted many players.
        \item<+-> The mathematician Gerolamo Cardano, in the middle of the 16th century, starts to investigate the odds of winning
        in a game of chance. He gives the first definition of the probability of an event as the ratio of
        the number of favorable outcomes to the total number of outcomes.
        \item<+-> One century later, Pierre de Fermat and Blaise Pascal lay the first theory of probability when
        answering a question from Antoine Gombaud, a famous Parisian gambler.
    \end{itemize}
\end{block}
\end{frame}
%2
\begin{frame}
    \frametitle{A bit of history}
\begin{block}{A modern view}
    \begin{itemize}
        \item<+-> In 1933, the Russian mathematician Andrey Kolmogorov gives the first axiomatic description of probability theory.
        \item<+-> This is the approach we will take in the sequel.
    \end{itemize}
\end{block}    
\begin{block}{Challenges}
    \begin{itemize}
       \item<+-> To fully understand Kolmogorov's contribution, we must consider cases in which counting the number of favorable outcomes is insufficient.
       \item<+-> For example, take a situation where the measure of interest is a real number, such as a current or a pressure.
       \item<+-> The value itself is not discrete; only the number of times it falls within a given interval can be observed.
       \item<+-> The Fermat-Pascal approach must be extended to these cases.
    \end{itemize}
\end{block}    
\end{frame}
%3
\begin{frame}
    \frametitle{From intuition to axioms }
\begin{block}{Sample space}
    \begin{itemize}
        \item<+-> Let's consider a very simple experiment: a coin is tossed, and the outcome observed.
        \item<+-> What are the possible results ?
        \item<+-> Of course, "head" and "tails" !
        \item<+-> The \textbf{sample space} of an experimnt is the set of all possible reults.
        \item<+-> In the coin tossing exemple, it is just the set $\Omega = \{ \text{heads}, \text{tails}\}.$
        \item<+-> If a die is rolled, the sample space is $\Omega = \{ 1,2,3,4,5, 6\}.$
        \item<+-> In a physical system, $\Omega$ may be a subset of the real numbers,
        or somethin even more complicated.
    \end{itemize}
\end{block}    
\end{frame}
%4 
\begin{frame}
    \frametitle{From intuition to axioms}
\begin{block}{Events}
    \begin{itemize}
        \item<+-> If two coins are tossed, the sample space is 
        $\Omega = \{ HH, HT, TH, TT \}.$
        \item<+-> An \textbf{event} is a statement about the outcome of the experiment, 
        like "at least one tails was observed".
        \item<+-> The previous event is the subset $\{HT,TH,TT\}$ of $\Omega.$
        \item<+-> In probability theory, an \textbf{event} is a subset of the \textbf{sample space}.
        \item<+-> The empty set $\emptyset$ and the sample space $\Omega$ are 
        always considered as events.
    \end{itemize}
\end{block}
\end{frame}
%5 
\begin{frame}
    \frametitle{From intuition to axioms}
\begin{block}{A bit of logic}
    \begin{itemize}
        \item<+-> Assertions about events are also events:
        \begin{itemize}
            \item<+-> If $A,B$ are events, so is $A \cup B.$
            \item<+-> If $A$ is an event, the complementary set $\bar{A}$ is an event.
        \end{itemize}
        \item<+-> For the two coins tossing experiment, if $A=\{HH\},B=\{HT\}$, then
         the event $A \cup B = \{HH, HT\}$ is "the first toss is a head", while the event
        $\bar{A}=\{HT,TH,TT\}$ is "at least one tails was observed".
    \end{itemize}
\end{block}
\begin{block}{Exercice}
    \begin{itemize}
        \item<+-> Find the sample space $\Omega$ associated with the roll of a die.
        \item<+-> What are the respective subsets $A,B$ of $\Omega$ corresponding to the observations
        "the result is greater than or equal to 3","the result is an even number".
        \item<+-> Describe the events $A\cup B$, $\bar{A}.$
    \end{itemize}
\end{block}
\end{frame}
%6
\begin{frame}
    \frametitle{Probabilities}
    \begin{block}{$\sigma$-algebras}
        \begin{itemize}
            \item<+-> Given a sample space $\Omega$, a $\sigma$-algebra of
            events is a set $\mathcal{T}$ of subsets from $\Omega$ such that:
            \begin{itemize}
                \item<+-> $\Omega \in \mathcal{T}.$
                \item<+-> If $A \in \mathcal{T}$, then $\bar{A} \in \mathcal{T}.$
                \item<+-> For any \textbf{countable} sequence $A_n$ in $\mathcal{T}$, the union
                $\bigcup_n A_n$ is in $\mathcal{T}.$
            \end{itemize}
        \end{itemize}
    \end{block}
\begin{block}{Probability measure}
    \begin{itemize}
        \item<+-> Given a $\sigma$-algebra $\mathcal{T}$ on $\Omega$, a 
        probability $P$ is a mapping that assigns to $A \in \mathcal{T}$ a real
        number $P(A) \in [0,1]$ such that:
        \begin{itemize}
        \item<+-> $P\left( \Omega \right) = 1.$
        \item<+-> For any countable sequence $A_n$ of \textbf{pairwise disjoint} events in $\mathcal{T}$,
        \begin{equation}
             \label{eq:proba_additivty}
            P\left( \bigcup_n A_n \right) = \sum_n P\left( A_n \right).
        \end{equation}
        \end{itemize}
    \end{itemize}
\end{block}
\end{frame}
%7
\begin{frame}
    \frametitle{Some properties}
\begin{block}{Probability space}
A sample space $\Omega$ together with a $\sigma$-algebra of events $\mathcal{T}$ and
a propability $P$ is called a \textbf{probability space}, denoted by 
$\left( \Omega, \mathcal{T}, P \right).$
\end{block}
\begin{block}{Set operations}
    Lat $\left( \Omega, \mathcal{T}, P \right)$ be a probability space. 
    Let $A,B$ be events. Then:
    \begin{itemize}
        \item<+-> $P\left(\bar{A} \right) = 1 - P(A).$
        \item<+-> $P\left( A \cup B \right) = P(A) + P(B) - P\left( A \cap B \right).$
        \item<+-> If $B \subset A$, then $P\left( A - B \right) = P(A) - P(B)$, where
        $A-B = A \cap \bar{B}.$
    \end{itemize}
\end{block}
\end{frame}
%8
\begin{frame}
    \frametitle{The die roll}
\begin{block}{Defining a Probability}
    \begin{itemize}
        \item<+-> $\Omega = \{1,2,3,4,5,6\}$, $\mathcal{T} = \mathcal{P}\left( \Omega \right)$, 
        the $\sigma$-algebra of all subsets. 
        \item<+-> How to find a probability on $\mathcal{T}$ ?
        \item<+-> It is required that $P\left( \Omega \right) = 1.$
        \item<+-> If the die is fair, then the probability to observe any number
        from $\Omega$ must be the same.
        \item<+-> Since:
        \begin{equation}
            1 = P\left( \Omega \right) = \sum_{i=1}^6 P\left( \{i\} \right),
        \end{equation}
        on deduce $P\left( \{i\} \right) = 1/6, i=1 \dots 6.$
        \item<+-> The probability of any event can then be obtained by summing the probabilities
        of its elements. 
    \end{itemize}
\end{block}
\end{frame}
%9
\begin{frame}
    \frametitle{Conditional probability}
    \begin{block}{Motivation}
    \begin{itemize}
        \item<+-> In many cases, there is an information about the outcome of
        an experiment.
        \item<+-> As an example, for a die roll, it may be "result is odd".
        \item<+-> The sample space is thus reduced, and the probabilities must 
        be rescaled accordingly.
        \item<+-> For the die example, teh sample space becomes $\{1,3,5\}$, and the probability 
        to draw a $3$ is $1/3.$
    \end{itemize}
    \end{block}
    \begin{block}{Bayes' formula}
        The contional probability of $A$ knowing $B$, denoted $P\left( A \vert B \right)$, is given by the Bayes' formula:
        \begin{equation}
            P\left( A \vert B  \right) = \frac{P\left( A \cap B \right)}{P(B)}.
        \end{equation}
    \end{block}
\end{frame}
%10
\begin{frame}
    \frametitle{Playing with conditional probabilities}

    

\end{frame}
